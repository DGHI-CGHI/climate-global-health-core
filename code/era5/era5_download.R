#!/usr/bin/env Rscript
################################################################################
# Project:   ERA5 Public S3 ??? Parallel Download ??? Regional NetCDF Subsetter
# File:      era5_download.R
# Version:   2025-09-29
# Author:    (you)
#
# PURPOSE
# =======
# This program automates three end-to-end steps for the NCAR public ERA5 archive
# hosted on AWS S3 at:  s3://nsf-ncar-era5   (anonymous, requestor-pays = NO)
#
#   1) Inventory:   Query the S3 bucket (ListBucket v2) for a Year-Month.
#   2) Download:    Fetch the specific NetCDF files for the variables you want,
#                   using parallel HTTP downloads with resumable transfers.
#   3) Subset:      For each downloaded file, clip the data to user-defined
#                   geographic bounding boxes (regions). For accumulated forecast
#                   variables (e.g., SSRD, TP, RO), convert from accumulated
#                   totals to hourly rates (first step ??? NA), and write out new
#                   NetCDF files per region.
#
# OUTPUT DIRECTORY STRUCTURE (as requested)
# ========================================
#   <OUT_REGION_DIR>/<Region>/<Year>/<Month>/<original-era5-filename>.nc
#
# Examples:
#   era5_regions/NorthAmerica/2004/05/e5.oper.fc.sfc.accumu.128_169_ssrd.ll025sc.2004050106_2004051606.nc
#   era5_regions/Africa/2001/01/e5.oper.an.sfc.128_167_2t.ll025sc.2001010100_2001013123.nc
#
# NOTE: There are no stream-specific subfolders in the region tree; the original
#       filename already contains the stream + variable shortname.
#
# DATASETS & VARIABLES
# ====================
# We support two ERA5 streams in this script:
#   . "e5.oper.an.sfc"         ??? instantaneous hourly analysis variables
#   . "e5.oper.fc.sfc.accumu"  ??? forecast-accumulated surface fluxes
#
# You configure the variables once in VAR_INDEX (shortname, canonical nc name,
# type=inst|accum, stream) and then choose your desired subset in WANT.
# The program matches files by shortname token in the S3 key (e.g., "_2t.",
# "_ssrd.", "_ro.") and also resolves the actual variable name inside NetCDF
# robustly (handles names like "t2m", "VAR_2T", "SSRD", etc.).
#
# ACCUMULATED ??? HOURLY CONVERSION
# ===============================
# For accumulated forecast variables, we:
#   . Difference along the time dimension (hourly cadence assumed).
#   . Convert units to rates:
#       - Energy fluxes (ssrd, sshf, slhf, e): J m??? per hour  ??? W m???  ( 3600)
#       - Depth  fluxes (tp, ro)              : m per hour     ??? mm hr??? ( 1000)
#   . The first time step becomes NA by definition after differencing.
#
# COORDINATES & LON WRAP
# ======================
# Files may have longitudes in [0, 360] or [-180, 180]. Bounding box selection
# automatically adapts and supports wrap-around selections (e.g., 170E ??? -160W).
# If the source latitudes are descending, the output is flipped to ascending.
#
# PARALLELISM
# ===========
# . Downloads:   parallel across files using {future.apply} with HTTP/2.
# . Subsetting:  parallel across files; within each file, each region is produced
#                sequentially (more stable, typically fast enough).
#
# ROBUSTNESS DETAILS
# ==================
# . ListBucket v2 over anonymous HTTPS, following region redirects.
# . Variable-name resolver checks several naming schemes and attributes.
# . Dimension handling defines ALL variable dims in the output, not just lon/lat/
#   time. This avoids "This dim has class: NULL" errors common with SSRD files.
# . Time dimension guesser recognizes "time", "valid_time", "forecast_time",
#   "utc_date", "step", etc., and verifies (optionally) that steps are roughly
#   hourly (warns if not).
#
# HOW TO USE
# ==========
# 0) Ensure packages are installed (xml2, curl, data.table, ncdf4, future,
#    future.apply). On Linux, libcurl/OpenSSL must support HTTPS.
# 1) Edit the USER CONFIG section below:
#       - OUT_DOWNLOAD_DIR, OUT_REGION_DIR
#       - REGIONS (bounding boxes)
#       - WANT (which variables to pull this run)
# 2) Run for a month:
#       Rscript era5_download_and_subset.R 2004 5
#
# TROUBLESHOOTING
# ===============
# . If parallel "multicore" is unstable (RStudio on Linux), the script falls
#   back to "multisession" automatically.
# . If you see 403 errors listing the S3 bucket, check for typos; the bucket is
#   public, and we do NOT send credentials.
# . If a particular file still fails in subsetting, share the filename; extending
#   the variable/dimension heuristics is straightforward.
# 
# COMMAND-LINE USAGE
# ==================
# Run for a single Year-Month, providing one root directory:
#   Rscript era5_download_and_subset.R <year> <month> --out=/path/to/run_root
#
# The script will create/use:
#   /path/to/run_root/era5_nc       (monthly downloads)
#   /path/to/run_root/era5_regions  (per-region subsets)
#   /path/to/run_root/wbgt          (WBGT outputs, if enabled)
#
# Examples:
#   Rscript era5_download_and_subset.R 2019 1 --out=/data/era5
#   Rscript era5_download_and_subset.R 2020 12 --out="D:/data/era5"   # Windows
#
# Notes:
# . Required args: <year> (integer), <month> (1-12).
# . If you pass relative paths, remember the script sets the working directory
#   later based on OS; prefer absolute paths for --out/--download-dir/--region-dir.
# . Downloads are parallelized; subsetting runs in parallel across files.
# . By default, the original monthly NetCDF files are deleted after subsetting.
#   Change delete_global=FALSE in subset_file_to_regions() if you want to keep them.

################################################################################

suppressPackageStartupMessages({
  library(xml2)
  library(curl)
  library(data.table)
  library(ncdf4)
  library(future)
  library(future.apply)
  library(digest)
  library(parallelly)
})

`%||%` <- function(a,b) if (!is.null(a)) a else b

# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#                         ?????? 1) USER CONFIGURATION ??????
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

# Where the *full* monthly source files are saved (mirrors S3 key path)
OUT_DOWNLOAD_DIR <- "era5_nc"

# Where the *regional* subset NetCDFs are written (region/year/month/files.nc)
OUT_REGION_DIR   <- "era5_regions"

# # Regions: bounding box per region (lon/lat in degrees). You can add more.
REGIONS <- list(
  "SEUS"   = list(lonmin = -100, lonmax =  -60, latmin =  15, latmax = 42),
  "SouthAmerica"   = list(lonmin =  -75, lonmax =  -34, latmin = -18, latmax = 10),
  "SoutheastAsia"  = list(lonmin =   68, lonmax = 130,  latmin = -5, latmax = 35), # include India & Sri Lanka
  "Africa"         = list(lonmin =  15, lonmax =   54, latmin = -37, latmax = 8) #38)
)

# Parallelism knobs
DOWNLOAD_WORKERS <- 6 # max(2L, parallel::detectCores() %/% 2)  # concurrent file downloads
SUBSET_WORKERS   <- 4 # max(2L, parallel::detectCores() - 1L)   # concurrent subsetting jobs
RETRIES          <- 3                                       # per-file retry count

# S3 public settings
BUCKET <- "nsf-ncar-era5"
BASE   <- sprintf("https://%s.s3.amazonaws.com", BUCKET)

# ERA5 streams used here
STREAM_AN       <- "e5.oper.an.sfc"         # instantaneous hourly analysis variables
STREAM_FC_ACCUM <- "e5.oper.fc.sfc.accumu"  # hourly accumulated (require differencing)
STREAM_MF       <- "e5.oper.fc.sfc.meanflux" #  mean flux stream for mtpr

# Instantaneous large-scale surface precipitaton fraction (fraction of grid cell precipitating...) not going to use.
# e5.oper.fc.sfc.instan    ilsf

# substitutes for lack of "total precipitation" field.
# e5.oper.fc.sfc.accumu FOR "cp" and "lsp"

# ?????? Master variable index ??????
VAR_INDEX <- data.table(
  variable = c(
    "2m_temperature",
    "2m_dewpoint_temperature",
    "10m_u_component_of_wind",
    "10m_v_component_of_wind",
    "volumetric_soil_water_layer_1",
    "surface_solar_radiation_downwards",
    "total_evaporation",
    "surface_sensible_heat_flux",
    "surface_latent_heat_flux",
    "runoff",  # NOTE: only "ro" (not sro/ssro)
    "mean_total_precipitation_rate"
    # convective_precipitation
    # stratiform_precipitation
  ),
  short    = c("2t","2d","10u","10v","swvl1","ssrd","e","sshf","slhf","ro","mtpr"),
  nc_name  = c("t2m","d2m","u10","v10","swvl1","ssrd","e","sshf","slhf","ro","mtpr"),
  type     = c("inst","inst","inst","inst","inst",
               "accum","accum","accum","accum","accum","inst"),
  stream   = c(STREAM_AN,STREAM_AN,STREAM_AN,STREAM_AN,STREAM_AN,
               STREAM_FC_ACCUM,STREAM_FC_ACCUM,STREAM_FC_ACCUM,
               STREAM_FC_ACCUM,STREAM_FC_ACCUM,
               STREAM_MF)
)[, rx := sprintf("\\_%s\\.", short)][]

# Choose variables for this run
WANT <-  c(
  "2m_temperature",
  "2m_dewpoint_temperature",
  "10m_u_component_of_wind",
  "10m_v_component_of_wind",
  "volumetric_soil_water_layer_1",
  "surface_solar_radiation_downwards",
  "total_evaporation",
  "surface_sensible_heat_flux",
  "surface_latent_heat_flux",
  "runoff",
  "mean_total_precipitation_rate"
)


# 
# extra <- data.table(
#   variable = c("large_scale_precip", "convective_precip", "snowfall"),
#   short    = c("lsp", "cp", "sf"),
#   nc_name  = c("lsp", "cp", "sf"),
#   type     = c("accum","accum","accum"),
#   stream   = STREAM_FC_ACCUM
# )[, rx := sprintf("\\_%s\\.", short)][]
# VAR_INDEX <- rbind(VAR_INDEX, extra, fill = TRUE)


CFG  <- VAR_INDEX[variable %in% WANT]


# WBGT calculation not available on repo yet.********************
# # ????????????????????????????????????????????????????????????????????????????????????
# # ?????? WBGT integration (post-subset) ????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
# # WBGT_SCRIPT      <- "era5_region_wbgt.R"  # path to your confirmed-working WBGT script
# WBGT_OUT_DIR     <- "wbgt"   # root where WBGT parquet files should go
# WBGT_RESOLUTION  <- "0.25"                # "0.25" for ERA5, "0.10" for ERA5-Land
# WBGT_WORKERS     <- 1 # max(1L, parallel::detectCores() - 1L)   # WBGT script's --workers
# WBGT_THREADS     <- 3 # max(1L, parallel::detectCores() - 2L)   # WBGT script's --wbgt-threads for jj::calcWBGT
# WBGT_OVERWRITE   <- FALSE                  # pass --overwrite if you want to rerun
# RUN_WBGT_AFTER_SUBSET <- TRUE              # master switch (set FALSE to skip WBGT stage)
# REMOVE_NC_REGION_FILES = TRUE              # remove after wbgt completes.



# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#          ?????? 2) FUTURE PLAN HELPER (portable multicore/multisession) ??????
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

plan_auto <- function(workers) {
  if (.Platform$OS.type == "windows") {
    future::plan(multisession, workers = workers)
  } else {
    ok <- FALSE
    try({
      ok <- parallelly::supportsMulticoreAndRStudio() && parallelly::supportsMulticore()
    }, silent = TRUE)
    if (isTRUE(ok)) future::plan(multicore, workers = workers) else future::plan(multisession, workers = workers)
  }
}

# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#                     ?????? 3) S3 URL + LISTBUCKET HELPERS ??????
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

s3_url <- function(bucket = BUCKET, key) {
  segs <- strsplit(key, "/", fixed = TRUE)[[1]]
  paste0("https://", bucket, ".s3.amazonaws.com/",
         paste(vapply(segs, curl::curl_escape, "", USE.NAMES = FALSE), collapse = "/"))
}
# stream = STREAM_FC_ACCUM
s3_list_month <- function(stream, year, month, max_keys = 1000L) {
  stopifnot(!grepl("/$", stream))
  prefix <- sprintf("%s/%04d%02d/", stream, year, month)
  token <- NULL; pieces <- list()
  repeat {
    qs <- list(`list-type`="2", `max-keys`=as.character(max_keys), prefix=prefix)
    if (!is.null(token)) qs[["continuation-token"]] <- token
    query <- paste(names(qs), vapply(qs, curl::curl_escape, "", USE.NAMES=FALSE), sep="=", collapse="&")
    url   <- paste0(BASE, "/?", query)
    
    h <- curl::new_handle()
    curl::handle_setopt(h, followlocation = TRUE, maxredirs = 10L, http_version = 2L)
    res <- curl::curl_fetch_memory(url, handle = h)
    doc <- xml2::read_xml(res$content)
    
    err <- xml_text(xml2::xml_find_first(doc, "/*[local-name()='Error']/*[local-name()='Code']"))
    if (!is.na(err) && nzchar(err)) {
      msg <- xml_text(xml2::xml_find_first(doc, "/*[local-name()='Error']/*[local-name()='Message']"))
      stop(sprintf("S3 error for %s: %s - %s", prefix, err, msg))
    }
    
    ks <- xml_text(xml2::xml_find_all(doc, ".//*[local-name()='Contents']/*[local-name()='Key']"))
    lm <- xml_text(xml2::xml_find_all(doc, ".//*[local-name()='Contents']/*[local-name()='LastModified']"))
    sz <- as.numeric(xml_text(xml2::xml_find_all(doc, ".//*[local-name()='Contents']/*[local-name()='Size']")))
    if (length(ks)) pieces[[length(pieces)+1L]] <- data.table(Key = ks,
                                                              LastModified = as.POSIXct(lm, tz="UTC"),
                                                              Size = sz)
    trunc_txt <- xml_text(xml2::xml_find_first(doc, ".//*[local-name()='IsTruncated']"))
    if (!identical(trunc_txt, "true")) break
    token <- xml_text(xml2::xml_find_first(doc, ".//*[local-name()='NextContinuationToken']"))
    if (!nzchar(token)) break
  }
  if (length(pieces)) rbindlist(pieces, use.names = TRUE) else data.table()
}

# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#                      ?????? 4) INVENTORY + FILTERING ??????
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

filter_by_cfg <- function(dt, cfg) {
  if (!nrow(dt) || !nrow(cfg)) return(data.table())
  out <- lapply(seq_len(nrow(cfg)), function(i) {
    nm <- cfg$variable[i]; rx <- cfg$rx[i]
    hit <- dt[grepl(rx, Key, ignore.case = TRUE)]
    if (nrow(hit)) hit[, variable := nm]
    hit
  })
  rbindlist(out, use.names = TRUE, fill = TRUE)
}

short_from_key <- function(k) {
  m <- regexpr("\\.(an|fc)\\.(pl|sfc|vinteg|invariant)\\.(\\d{3})_(\\d{3})_([a-z0-9]+)\\.", basename(k), perl=TRUE)
  if (m < 0) return(NA_character_)
  sub("^.*_(?:\\d{3})_([a-z0-9]+)\\.$", "\\1", regmatches(basename(k), m))
}

get_month_inventory <- function(year, month, cfg = CFG) {
  an_cfg <- cfg[stream == STREAM_AN]
  fc_cfg <- cfg[stream == STREAM_FC_ACCUM]
  mf_cfg <- cfg[stream == STREAM_MF]
  
  an_list <- if (nrow(an_cfg)) s3_list_month(STREAM_AN, year, month) else data.table()
  fc_list <- if (nrow(fc_cfg)) s3_list_month(STREAM_FC_ACCUM, year, month) else data.table()
  mf_list <- if (nrow(fc_cfg)) s3_list_month(STREAM_MF, year, month) else data.table()
  
  an_need <- if (nrow(an_cfg)) filter_by_cfg(an_list, an_cfg) else data.table()
  fc_need <- if (nrow(fc_cfg)) filter_by_cfg(fc_list, fc_cfg) else data.table()
  mf_need <- if (nrow(mf_cfg)) filter_by_cfg(mf_list, mf_cfg) else data.table()
  
  inv <- rbindlist(list(an_need, fc_need, mf_need), use.names = TRUE, fill = TRUE)
  if (!nrow(inv)) return(inv)
  
  inv <- merge(inv, cfg[, .(variable, short, nc_name, type, stream)], by = "variable", all.x = TRUE)
  inv[, `:=`(year = year, month = month, file_short = vapply(Key, short_from_key, ""))]
  unique(inv, by = "Key")
}

# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#                ?????? 5) PARALLEL DOWNLOADER (RESUMABLE) ??????
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

parallel_download <- function(keys, out_dir = OUT_DOWNLOAD_DIR,
                              workers = DOWNLOAD_WORKERS, retries = RETRIES, verbose = TRUE) {
  if (!length(keys)) return(data.table())
  dests <- file.path(out_dir, keys)
  vapply(unique(dirname(dests)), dir.create, FALSE, recursive = TRUE, showWarnings = FALSE)
  
  plan0 <- future::plan()
  on.exit(future::plan(plan0), add = TRUE)
  plan_auto(workers)
  
  res <- future_lapply(
    seq_along(keys),
    function(i) {
      k <- keys[i]; d <- dests[i]; u <- s3_url(BUCKET, k)
      tries <- 0L
      repeat {
        tries <- tries + 1L
        h <- curl::new_handle()
        curl::handle_setopt(
          h,
          followlocation = TRUE, maxredirs = 10L, http_version = 2L,
          tcp_nodelay = TRUE, low_speed_time = 30L, low_speed_limit = 10000L,
          connecttimeout = 30L, timeout = 0L,  # no overall timeout (monthly files can be big)
          useragent = "era5-ncar-s3/1.0"
        )
        if (file.exists(d)) {
          sz <- file.info(d)$size
          if (!is.na(sz) && sz > 0) curl::handle_setopt(h, resume_from = as.numeric(sz))
        } else {
          dir.create(dirname(d), recursive = TRUE, showWarnings = FALSE)
        }
        ok <- tryCatch({ curl::curl_download(u, d, mode = "wb", handle = h, quiet = !verbose); TRUE },
                       error = function(e) FALSE)
        if (ok && file.info(d)$size > 0) return(list(ok=TRUE, key=k, dest=d, tries=tries))
        if (tries >= retries)           return(list(ok=FALSE, key=k, dest=d, tries=tries))
        Sys.sleep(1 + tries) # backoff
      }
    },
    future.seed = TRUE,
    future.chunk.size = 1,   # dynamic queue: one task per future
    future.scheduling = Inf
  )
  
  rbindlist(lapply(res, as.data.frame), fill = TRUE)
}

# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#                     ?????? 6) SUBSETTING UTILITIES ??????
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

.to360 <- function(x) ifelse(x < 0, x + 360, x)

lon_index <- function(lons, lonmin, lonmax) {
  rng <- range(lons, na.rm = TRUE)
  if (rng[1] >= 0 && rng[2] <= 360) { a <- .to360(lonmin); b <- .to360(lonmax) } else { a <- lonmin; b <- lonmax }
  if (a <= b) which(lons >= a & lons <= b) else c(which(lons >= a), which(lons <= b))
}

guess_lon_name <- function(nc) {
  cand <- c("longitude","lon","lons","grid_longitude","longitude_0","x","X")
  nm <- intersect(cand, c(names(nc$var), names(nc$dim)))[1]
  nm %||% "longitude"
}
guess_lat_name <- function(nc) {
  cand <- c("latitude","lat","lats","grid_latitude","latitude_0","y","Y")
  nm <- intersect(cand, c(names(nc$var), names(nc$dim)))[1]
  nm %||% "latitude"
}

choose_varname <- function(nc, cfg_row) {
  vars <- names(nc$var)
  if (!length(vars)) return(NA_character_)
  
  short <- as.character(cfg_row$short %||% cfg_row[["short"]])
  canon <- as.character(cfg_row$nc_name %||% cfg_row[["nc_name"]])
  
  cands <- unique(na.omit(c(
    canon, short, toupper(short), tolower(short),
    paste0("VAR_", toupper(short)), paste0("var_", tolower(short))
  )))
  hit <- cands[cands %in% vars]
  if (length(hit)) return(hit[1])
  
  tolower_vec <- function(x) { x <- as.character(x); x[is.na(x)] <- ""; tolower(x) }
  needle <- unique(tolower_vec(c(short, canon))); needle <- needle[nzchar(needle)]
  for (v in vars) {
    atts_raw <- tryCatch({
      c(
        ncatt_get(nc, v, "GRIB_shortName")$value,
        ncatt_get(nc, v, "short_name")$value,
        ncatt_get(nc, v, "standard_name")$value,
        ncatt_get(nc, v, "long_name")$value
      )
    }, error = function(e) character())
    atts <- tolower_vec(atts_raw); atts <- atts[nzchar(atts)]
    if (length(atts) && length(needle) && any(needle %in% atts)) return(v)
  }
  
  nonmeta <- setdiff(vars, c("time","valid_time","forecast_time","utc_date","lat","latitude","lon","longitude","step"))
  if (length(nonmeta) >= 1) return(nonmeta[1])
  NA_character_
}

diff_along <- function(arr, tpos) {
  nd <- length(dim(arr))
  if (nd == 1L) return(c(NA, diff(arr)))
  ord <- c(setdiff(seq_len(nd), tpos), tpos)
  inv <- match(seq_len(nd), ord)
  A <- aperm(arr, ord)
  d <- dim(A)
  if (d[nd] < 2L) return(array(NA_real_, dim = d))
  out <- array(NA_real_, dim = d)
  out_idx <- replicate(nd, TRUE, simplify = FALSE); out_idx[[nd]] <- 2:d[nd]
  a1_idx  <- replicate(nd, TRUE, simplify = FALSE); a1_idx[[nd]] <- 2:d[nd]
  a0_idx  <- replicate(nd, TRUE, simplify = FALSE); a0_idx[[nd]] <- 1:(d[nd]-1)
  out <- do.call(`[<-`, c(list(out), out_idx,
                          list(do.call(`[`, c(list(A), a1_idx)) - do.call(`[`, c(list(A), a0_idx)))))
  aperm(out, inv)
}

find_time_axis <- function(nc) {
  dim_names <- names(nc$dim)
  cand_dim <- intersect(c("time","valid_time","forecast_time"), dim_names)
  if (length(cand_dim)) {
    tname <- cand_dim[1]
    if (tname %in% names(nc$var)) {
      tvals <- tryCatch(ncvar_get(nc, tname), error = function(e) NULL)
      tunits <- tryCatch(ncatt_get(nc, tname, "units")$value, error = function(e) NULL)
      tcal   <- tryCatch(ncatt_get(nc, tname, "calendar")$value, error = function(e) NULL)
    } else {
      tvals <- nc$dim[[tname]]$vals
      tunits <- NULL
      tcal <- NULL
    }
    return(list(name = tname,
                vals = tvals,
                units = if (is.null(tunits) || !nzchar(tunits)) "hours since 1900-01-01 00:00:00" else tunits,
                cal = tcal %||% "standard"))
  }
  if ("utc_date" %in% names(nc$var)) {
    ud <- ncvar_get(nc, "utc_date")
    ud <- as.character(ud)            # "YYYYMMDDHH"
    tposix <- as.POSIXct(ud, format = "%Y%m%d%H", tz = "UTC")
    origin <- as.POSIXct("1900-01-01 00:00:00", tz = "UTC")
    hrs <- as.numeric(difftime(tposix, origin, units = "hours"))
    return(list(name = "time", vals = hrs,
                units = "hours since 1900-01-01 00:00:00", cal = "standard"))
  }
  list(name = NULL, vals = NULL, units = "hours since 1900-01-01 00:00:00", cal = "standard")
}

# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#                      ?????? 7) CORE: SUBSET ONE FILE ??????
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

subset_nc_bbox <- function(infile, outfile, bbox, meta_row, convert_units = TRUE, verify_time = TRUE) {
  infile <- as.character(infile)[1]
  if (!is.character(infile) || !nzchar(infile)) stop("subset_nc_bbox: bad infile scalar")
  if (!file.exists(infile)) stop("subset_nc_bbox: infile does not exist: ", infile)
  
  nc <- try(ncdf4::nc_open(infile), silent = TRUE)
  if (inherits(nc, "try-error")) stop("nc_open failed for: ", infile, " :: ", as.character(nc))
  on.exit(try(ncdf4::nc_close(nc), silent = TRUE), add = TRUE)
  
  vname <- choose_varname(nc, meta_row)
  if (!is.character(vname) || !nzchar(vname)) {
    stop(sprintf("Unable to resolve variable for '%s' in %s. Available: %s",
                 as.character(meta_row$variable %||% meta_row[["variable"]] %||% "NA"),
                 basename(infile), paste(names(nc$var), collapse = ", ")))
  }
  vinfo <- nc$var[[vname]]
  
  lon_name <- guess_lon_name(nc); lat_name <- guess_lat_name(nc)
  lons <- ncvar_get(nc, lon_name)
  lats <- ncvar_get(nc, lat_name)
  
  ii <- lon_index(lons, bbox$lonmin, bbox$lonmax)
  lat_desc <- (length(lats) > 1 && lats[2] < lats[1])
  jj <- if (lat_desc) which(lats <= bbox$latmax & lats >= bbox$latmin) else which(lats >= bbox$latmin & lats <= bbox$latmax)
  if (!length(ii) || !length(jj)) stop("Empty bbox selection for ", basename(infile))
  lons_sub <- lons[ii]
  lats_sub <- if (lat_desc) sort(lats[jj]) else lats[jj]
  
  tinfo <- find_time_axis(nc)
  
  dim_names <- vapply(vinfo$dim, function(d) d$name, "")
  dim_lens  <- vapply(vinfo$dim, function(d) d$len,  integer(1))
  pos_lon   <- match(lon_name, dim_names)
  pos_lat   <- match(lat_name, dim_names)
  pos_time  <- if (!is.null(tinfo$name)) match(tinfo$name, dim_names) else NA_integer_
  
  ii_contig <- if (!is.na(pos_lon)) ((max(ii) - min(ii) + 1L) == length(ii)) else TRUE
  start <- rep(1L, length(dim_names))
  count <- dim_lens
  if (!is.na(pos_lat))  { start[pos_lat]  <- min(jj); count[pos_lat]  <- length(jj) }
  if (!is.na(pos_time)) { start[pos_time] <- 1L;     count[pos_time] <- dim_lens[pos_time] }
  
  if (!is.na(pos_lon) && ii_contig) {
    start[pos_lon] <- min(ii); count[pos_lon] <- length(ii)
    arr <- ncvar_get(nc, vname, start = start, count = count)
  } else {
    if (!is.na(pos_lon)) { start[pos_lon] <- 1L; count[pos_lon] <- dim_lens[pos_lon] }
    arr <- ncvar_get(nc, vname, start = start, count = count)
    if (!is.na(pos_lon)) {
      idx <- replicate(length(dim_names), TRUE, simplify = FALSE)
      idx[[pos_lon]] <- ii
      arr <- do.call(`[`, c(list(arr), idx, list(drop = FALSE)))
    }
  }
  
  if (lat_desc && !is.na(pos_lat)) {
    idx <- replicate(length(dim_names), TRUE, simplify = FALSE)
    idx[[pos_lat]] <- rev(seq_len(length(jj)))
    arr <- do.call(`[`, c(list(arr), idx, list(drop = FALSE)))
  }
  
  is_accum <- identical(as.character(meta_row$type %||% meta_row[["type"]]), "accum")
  if (is_accum && !is.na(pos_time)) {
    arr <- diff_along(arr, pos_time)
  }
  
  d_defs <- vector("list", length(dim_names))
  for (k in seq_along(dim_names)) {
    nm <- dim_names[k]
    if (!is.na(pos_lon)  && k == pos_lon)  d_defs[[k]] <- ncdim_def(lon_name, "degrees_east",  vals = lons_sub)
    else if (!is.na(pos_lat)  && k == pos_lat)  d_defs[[k]] <- ncdim_def(lat_name, "degrees_north", vals = lats_sub)
    else if (!is.na(pos_time) && k == pos_time) {
      tvals <- tinfo$vals
      if (is.null(tvals) || length(tvals) != dim_lens[pos_time]) tvals <- seq_len(dim_lens[pos_time]) - 1L
      tunits <- tinfo$units
      d_defs[[k]] <- ncdim_def("time", tunits, vals = tvals, unlim = FALSE)
    } else {
      d_defs[[k]] <- ncdim_def(nm, "", vals = seq_len(dim_lens[k]))
    }
  }
  
  var_units <- ncatt_get(nc, vname, "units")$value %||% ""
  var_long  <- ncatt_get(nc, vname, "long_name")$value %||% vname
  if (is_accum) {
    s <- tolower(as.character(meta_row$short %||% meta_row[["short"]]))
    if (convert_units) {
      if (s %in% c("ssrd","sshf","slhf","e")) { arr <- arr / 3600; var_units <- "W m-2" }
      if (s %in% c("tp","ro"))               { arr <- arr * 1000; var_units <- "mm hr-1" }
    }
  }
  
  dir.create(dirname(outfile), recursive = TRUE, showWarnings = FALSE)
  v_out <- ncvar_def(vname, var_units, d_defs, missval = 9.96921e36, prec = "float", longname = var_long)
  oncn <- nc_create(outfile, vars = list(v_out), force_v4 = TRUE)
  on.exit(try(nc_close(oncn), silent = TRUE), add = TRUE)
  
  ncvar_put(oncn, vname, arr)
  
  ncatt_put(oncn, 0, "source_file", basename(infile))
  ncatt_put(oncn, 0, "subset_bbox",
            sprintf("lon[%g,%g], lat[%g,%g]", bbox$lonmin, bbox$lonmax, bbox$latmin, bbox$latmax))
  invisible(outfile)
}

# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#                  ?????? 8) SUBSET A DOWNLOADED FILE TO ALL REGIONS ??????
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

subset_file_to_regions <- function(infile, key, meta_row,
                                   out_root = OUT_REGION_DIR,
                                   regions = REGIONS,
                                   delete_global = TRUE) {
  infile <- as.character(infile)[1]
  key    <- as.character(key)[1]
  stopifnot(is.character(infile), nzchar(infile), file.exists(infile))
  stopifnot(is.character(key),    nzchar(key))
  stopifnot(is.list(meta_row))
  stopifnot(is.character(out_root), nzchar(out_root))
  stopifnot(length(regions) > 0L)
  
  yyyymm <- basename(dirname(key))
  stopifnot(grepl("^\\d{6}$", yyyymm))
  year   <- substr(yyyymm, 1, 4)
  month  <- substr(yyyymm, 5, 6)
  short  <- as.character(meta_row$short %||% meta_row[["short"]])
  
  base <- basename(key)
  tag  <- if (grepl("\\d{10}_\\d{10}", base)) {
    sub(".*?(\\d{10})_(\\d{10}).*", "\\1-\\2", base)
  } else {
    paste0("part-", substr(digest::digest(base, algo = "xxhash64"), 1, 8))
  }
  
  res <- lapply(names(regions), function(rname) {
    bbox   <- regions[[rname]]
    outdir <- file.path(out_root, rname, year, month)
    dir.create(outdir, recursive = TRUE, showWarnings = FALSE)
    
    outfile <- file.path(outdir, sprintf("%s_%s_%s.nc", short, yyyymm, tag))
    message(sprintf("[subset] %-4s ??? %-12s : %s", short, rname, basename(outfile)))
    
    subset_nc_bbox(infile, outfile, bbox, meta_row = meta_row, convert_units = TRUE, verify_time = TRUE)
    outfile
  })
  
  if (isTRUE(delete_global)) try(unlink(infile), silent = TRUE)
  invisible(res)
}

# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#                          ?????? 9) ORCHESTRATION ??????
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

run_month <- function(year, month, cfg = CFG, dl_workers = DOWNLOAD_WORKERS, subset_workers = SUBSET_WORKERS) {
  # jj::timed('start')
  inv <- get_month_inventory(year, month, cfg)
  
  dl_workers = nrow(inv)
  
  if (!nrow(inv)) { message(sprintf("[inv] no files for %04d-%02d", year, month)); return(invisible(NULL)) }
  
  keys <- inv$Key
  message(sprintf("[download] %d files for %04d-%02d (workers=%d)", length(keys), year, month, dl_workers))
  dl <- parallel_download(keys, workers = dl_workers)
  
  ok <- merge(inv, dl[ok == TRUE, .(Key = key, dest)], by = "Key")
  if (!nrow(ok)) { message("[download] none succeeded"); return(invisible(NULL)) }
  
  # Drop NA/bad paths and de-duplicate tasks
  ok <- ok[!is.na(dest) & nzchar(dest)]
  ok <- unique(ok, by = c("dest","Key"))
  
  message(sprintf("[subset] %d files ??? regions (workers=%d)", nrow(ok), subset_workers))
  # jj::timed('end')

  #####################################################
  #####################################################
  #####################################################
  # subset_workers = min(4, nrow(ok))
  subset_workers = nrow(ok)
  
  # jj::timed('start')
  # Parallel across files only; pass simple list meta_row to avoid DT quirks
  plan0 <- future::plan(); on.exit(future::plan(plan0), add = TRUE); plan_auto(subset_workers)
  future_lapply(
    seq_len(nrow(ok)),
    function(ii) {
      infile <- as.character(ok$dest[[ii]])
      ky     <- as.character(ok$Key[[ii]])
      meta_row <- list(
        short    = as.character(ok$short[[ii]]),
        nc_name  = as.character(ok$nc_name[[ii]]),
        type     = as.character(ok$type[[ii]]),
        variable = as.character(ok$variable[[ii]])
      )
      if (!is.character(infile) || length(infile) != 1L || !nzchar(infile))
        stop("Bad infile scalar for row ", ii, ": ", paste(infile, collapse="; "))
      if (!file.exists(infile))
        stop("Input file not found for row ", ii, ": ", infile)
      
      subset_file_to_regions(infile, ky, meta_row = meta_row)
      TRUE
    },
    future.seed = TRUE,
    future.chunk.size = 1,
    future.scheduling = Inf
  )
  # jj::timed('end')
  # #####################################################
  # #####################################################
  # Calculation of WBGT not available in repo currently.
  # #####################################################
  # # ?????? WBGT after subsetting: parallel over regions with 2 workers ??????????????????????????????????????????
  # DO_WBGT_AFTER_SUBSET <- TRUE
  # 
  # if (isTRUE(DO_WBGT_AFTER_SUBSET)) {
  #   # Keep heavy math libs from oversubscribing each worker
  #   Sys.setenv(OMP_NUM_THREADS = "1", MKL_NUM_THREADS = "1", OPENBLAS_NUM_THREADS = "1")
  #   
  #   plan0 <- future::plan(); on.exit(future::plan(plan0), add = TRUE)
  #   plan_auto(workers = 4)  # ??? exactly 2/4 workers
  #   
  #   # jj::timed('start')
  #   
  #   res <- future.apply::future_lapply(
  #     names(REGIONS),
  #     FUN = function(rname) {
  #       rdir <- file.path(OUT_REGION_DIR, rname, sprintf("%04d", year), sprintf("%02d", month))
  #       ncs  <- if (dir.exists(rdir)) list.files(rdir, pattern = "\\.nc$", full.names = TRUE) else character()
  #       if (!length(ncs)) {
  #         msg <- sprintf("[wbgt] %s %04d-%02d : no .nc files, skipping", rname, year, month)
  #         message(msg)
  #         return(msg)
  #       }
  #       
  #       message(sprintf("[wbgt] %s %04d-%02d : starting", rname, year, month))
  #       
  #       # IMPORTANT: if run_wbgt_for_region_month_remote() does its own parallelism,
  #       # make sure it uses sequential execution OR threads=1 to avoid nested workers.
  #       run_wbgt_for_region_month_remote(rname, year, month)
  #       
  #       if (isTRUE(REMOVE_NC_REGION_FILES)) {
  #         # remove only this region's files, *after* its WBGT completes
  #         try(unlink(ncs), silent = TRUE)
  #       }
  #       msg <- sprintf("[wbgt] %s %04d-%02d : done", rname, year, month)
  #       message(msg)
  #       msg
  #     },
  #     future.seed = TRUE,
  #     future.chunk.size = 1,   # one region per task
  #     future.scheduling = Inf  # refill the queue as soon as a worker frees up
  #   )
  #   
  #   # jj::timed('end')
  # }
  
  invisible(TRUE)
}

# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#                     ?????? WBGT post-processing launcher ??????
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
run_wbgt_for_region_month <- function(region, year, month,
                                      subset_root = OUT_REGION_DIR,
                                      wbgt_script = WBGT_SCRIPT_PATH,
                                      out_dir     = WBGT_OUT_DIR,
                                      threads     = WBGT_THREADS,
                                      workers     = WBGT_WORKERS,
                                      resolution  = WBGT_RESOLUTION,
                                      verbose     = TRUE) {
  # Ensure paths exist/are normalized
  subset_root <- normalizePath(subset_root, winslash = "/", mustWork = TRUE)
  out_dir     <- normalizePath(out_dir,     winslash = "/", mustWork = FALSE)
  dir.create(out_dir, recursive = TRUE, showWarnings = FALSE)
  
  # Your WBGT script already has a robust CLI; call it directly.
  # We execute for a single (region, year, month).
  # NOTE: The script accepts range forms; we'll pass singletons.
  args <- c(
    wbgt_script,
    sprintf("--subset-root=%s", subset_root),
    sprintf("--region=%s",      region),
    sprintf("--years=%d",     year),
    sprintf("--months=%d",    as.integer(month)),
    sprintf("--out-dir=%s",     out_dir),
    sprintf("--resolution=%s",  resolution),
    sprintf("--workers=%d",     workers),
    sprintf("--wbgt-threads=%d",threads),
    "--verbose"                 # keep logs on
    # ,"--overwrite"            # uncomment if you want to always recompute
  )
  
  if (verbose) {
    message(sprintf("[wbgt] %s %04d-%02d ??? %s", region, year, as.integer(month), out_dir))
  }
  # Launch Rscript as a separate process to keep responsibilities clean
  status <- suppressWarnings(system2(command = R.home("bin/Rscript"), args = args))
  
  if (status != 0) warning(sprintf("WBGT step failed for %s %04d-%02d (exit=%d)", region, year, as.integer(month), status))
  invisible(status == 0)
}




# Ensure dynamic scheduling globally, just in case
options(future.chunk.size = 1, future.scheduling = Inf)


# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#                               ?????? 10) CLI ??????
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#                               ?????? 10) CLI ??????
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#                               ?????? 10) CLI ??????
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

parse_cli <- function(args) {
  usage <- paste(
    "Usage:",
    "  Rscript era5_download_and_subset.R <year> <month> --out=DIR",
    "",
    "Required arguments:",
    "  <year>     Integer (e.g., 2020)",
    "  <month>    Integer 1..12 (e.g., 7)",
    "  --out=DIR  Root directory for this run.",
    "             Subdirectories will be created automatically:",
    "               DIR/era5_nc       (monthly downloads)",
    "               DIR/era5_regions  (per-region subsets)",
    "               DIR/wbgt          (WBGT outputs, if enabled)",
    sep = "\n"
  )
  
  if (length(args) < 2) stop(usage, call. = FALSE)
  
  year  <- suppressWarnings(as.integer(args[1]))
  month <- suppressWarnings(as.integer(args[2]))
  if (is.na(year) || is.na(month) || month < 1 || month > 12) stop(usage, call. = FALSE)
  
  opts <- args[-c(1,2)]
  get_opt <- function(key) {
    hit <- grep(paste0("^", key, "="), opts, value = TRUE)
    if (length(hit)) sub(paste0("^", key, "="), "", hit[1]) else NULL
  }
  
  out_root <- get_opt("--out")
  if (is.null(out_root) || !nzchar(out_root)) stop(usage, call. = FALSE)
  
  list(
    year     = year,
    month    = month,
    out_root = out_root
  )
}

ensure_dirs <- function(...) {
  paths <- list(...)
  for (p in paths) dir.create(p, recursive = TRUE, showWarnings = FALSE)
  invisible(paths)
}

args <- commandArgs(trailingOnly = TRUE)
if (length(args) >= 2) {
  cli <- parse_cli(args)
  
  # Derive all outputs from a single root
  OUT_DOWNLOAD_DIR <<- file.path(cli$out_root, "era5_nc")
  OUT_REGION_DIR   <<- file.path(cli$out_root, "era5_regions")
  WBGT_OUT_DIR     <<- file.path(cli$out_root, "wbgt")   # optional but consistent
  
  ensure_dirs(OUT_DOWNLOAD_DIR, OUT_REGION_DIR, WBGT_OUT_DIR)
  
  message(sprintf("[root ] out          = %s", normalizePath(cli$out_root, winslash = "/", mustWork = FALSE)))
  message(sprintf("[paths] download_dir = %s", normalizePath(OUT_DOWNLOAD_DIR, winslash = "/", mustWork = FALSE)))
  message(sprintf("[paths] region_dir   = %s", normalizePath(OUT_REGION_DIR,   winslash = "/", mustWork = FALSE)))
  message(sprintf("[paths] wbgt_dir     = %s", normalizePath(WBGT_OUT_DIR,     winslash = "/", mustWork = FALSE)))
  
  run_month(cli$year, cli$month)
  
} else {
  message("Usage: Rscript era5_download_and_subset.R <year> <month> --out=DIR")
}


