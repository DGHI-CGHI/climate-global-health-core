#!/usr/bin/env Rscript
################################################################################
# ERA5 ??? Census Geographies: Arrow + C++ (hourly & daily)
# Author: you
# Last Updated: 2025-10-17
#
# WHAT THIS DOES
#   . Reads hourly ERA5 parquet (one folder per year)
#   . (Optionally) builds grid???geography area weights (exactextractr)
#   . Hourly aggregation per geography using your C++ function(s)
#   . Daily aggregation per geography using your C++ function(s)
#
# 
# ********************
#
# NOTE: Currently, hourly era5 data utilized by this script IS NOT in an 
        # accessible source to reproduce output of code below
# 
# ********************
# 
#
#
# CLI (Windows PowerShell example):
# Rscript "C:/.../era5_census_cli.R" `
#   --out="C:/.../era5_agg_out/" `
#   --data-dir="E:/data/gridded/era5land-northcarolina/processed" `
#   --geog-shp="/path/to/shapefile/US_county_2023.shp" `
#   --geog-name=county `
#   --state-fips=37 `
#   --years=2025 `
#   --months=5 `
#   --vars="name=wbgt,col=wbgt_scaled10_degC,scale=10,unit=degC;name=ta,col=ta_scaled10_degC,scale=10,unit=degC" `
#   --lat-col=lat_idx_x10 `
#   --lon-col=lon_idx_x10 `
#   --lat-scale=10 `
#   --lon-scale=10
################################################################################

suppressPackageStartupMessages({
  library(data.table)
  library(dplyr)
  library(arrow)
  library(lubridate)
  library(sf)
  library(terra)
  library(exactextractr)
  library(lutz)
  requireNamespace("Rcpp")
})

# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#  Rcpp: load your fast hourly & daily aggregators
#    . hourly_stats_cpp_multi(vars=list(...), weight=..., group=..., var_names=c(...))
#    . daily_stats_cpp(x, group_id)  ??? list(mean=, min=, max=)
# Adjust RCPP_DIR / filenames to where your C++ lives.
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
# Define project-relative directories
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

RCPP_DIR <- here::here("code", "helpers")
if (!dir.exists(RCPP_DIR)) {
  # fallback: find directory of current script
  this_dir <- dirname(normalizePath(sys.frames()[[1]]$ofile, mustWork = FALSE))
  RCPP_DIR <- file.path(this_dir, "helpers")
}
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
# Load C++ source files (relative to repo)
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
Rcpp::sourceCpp(file.path(RCPP_DIR, "calcHourlyStatsWeighted2.cpp"))
Rcpp::sourceCpp(file.path(RCPP_DIR, "daily_stats_cpp.cpp"))

# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
# Helpers
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????

# Parse --vars string:  "name=wbgt,col=wbgt_scaled10_degC,scale=10,unit=degC;name=ta,col=ta_scaled10_degC,scale=10,unit=degC"
parse_vars <- function(x) {
  if (is.null(x) || !nzchar(x)) stop("--vars is required")
  items <- strsplit(x, ";", fixed = TRUE)[[1]]
  rows <- lapply(items, function(s) {
    kv <- strsplit(s, ",", fixed = TRUE)[[1]]
    out <- as.list(setNames(rep(NA_character_, 4), c("name","col","scale","unit")))
    for (pair in kv) {
      a <- strsplit(pair, "=", fixed = TRUE)[[1]]
      if (length(a) == 2) out[[a[1]]] <- a[2]
    }
    out$scale <- as.numeric(out$scale)
    out
  })
  as.data.table(rows)
}

# Arrow time-window helper: aligns literal types with column type
arrow_time_window <- function(ds, time_col = "validTime", start_posixct, end_posixct) {
  sc <- schema(ds); tf <- sc[[time_col]]
  if (is.null(tf)) stop("Column '", time_col, "' not found in dataset.")
  ttype <- tf$type$ToString()
  
  do_filter <- function(.ds, colname, lo, hi) {
    .ds %>% dplyr::filter(
      !!rlang::sym(colname) >= !!arrow::scalar(lo),
      !!rlang::sym(colname) <  !!arrow::scalar(hi)
    )
  }
  
  if (grepl("^timestamp\\[", ttype)) {
    ds %>% do_filter(time_col, start_posixct, end_posixct)
  } else if (grepl("^int(32|64)$", ttype)) {
    ms <- as.integer(as.numeric(start_posixct))
    me <- as.integer(as.numeric(end_posixct))
    ds %>%
      dplyr::mutate(!!time_col := arrow::cast(!!rlang::sym(time_col), arrow::int64())) %>%
      dplyr::filter(
        !!rlang::sym(time_col) >= !!arrow::scalar(ms),
        !!rlang::sym(time_col) <  !!arrow::scalar(me)
      )
  } else {
    stop("Unsupported time type for validTime: ", ttype)
  }
}

ensure_dirs <- function(...) {
  p <- file.path(...)
  dir.create(p, recursive = TRUE, showWarnings = FALSE)
  p
}

round_latlon <- function(x) round(x, 4)  # harmonize with weights

# right before pick_idx_col() calls in build_grid_weights()
normalize_idx_name <- function(x) {
  if (is.null(x) || !nzchar(x)) return(x)
  sub("_x10$", "_10", x)   # map the common variant to actual
}
# ---- Resolve index column names
pick_idx_col <- function(prefer, schema_names, patterns) {
  if (!is.null(prefer) && nzchar(prefer) && prefer %in% schema_names) return(prefer)
  # try patterns in order
  for (rx in patterns) {
    hits <- grep(rx, schema_names, value = TRUE, ignore.case = TRUE)
    # prefer canonical "lat_idx_10" / "lon_idx_10" if present
    if (length(hits)) {
      ord <- match(c("lat_idx_10","lon_idx_10","lat_idx_x10","lon_idx_x10"), hits)
      if (any(!is.na(ord))) hits <- hits[order(replace(ord, is.na(ord), 999))]
      return(hits[1])
    }
  }
  return(NA_character_)
}


open_dataset_root <- function(root) {
  # Works whether root points at a directory of year subdirs or a flat parquet tree
  if (dir.exists(root)) {
    arrow::open_dataset(root, format = "parquet")
  } else {
    stop("Data dir does not exist: ", root)
  }
}




resolve_index_columns <- function(cfg, ds_schema_names) {
  # Normalize the provided names (e.g., *_x10 ??? *_10)
  cfg$lat_col <- normalize_idx_name(cfg$lat_col)
  cfg$lon_col <- normalize_idx_name(cfg$lon_col)
  
  # Try to pick existing columns
  lat_used <- pick_idx_col(
    cfg$lat_col,
    ds_schema_names,
    patterns = c("^lat_idx(_x?10)?$", "^lat_idx_?10$", "lat_idx", "lat.*idx", "^lat$")
  )
  lon_used <- pick_idx_col(
    cfg$lon_col,
    ds_schema_names,
    patterns = c("^lon_idx(_x?10)?$", "^lon_idx_?10$", "lon_idx", "lon.*idx", "long.*idx", "^lon$")
  )
  
  if (is.na(lat_used) || is.na(lon_used)) {
    stop(
      "Required index columns not found. ",
      "Looked for cfg$lat_col='", cfg$lat_col, "' / cfg$lon_col='", cfg$lon_col, "'; ",
      "schema columns include: ", paste(ds_schema_names, collapse=", "), " ."
    )
  }
  list(lat_col = lat_used, lon_col = lon_used)
}


# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
# 1) Build grid???geography weights (area-based). Saves RDS path; returns it.
#    The RDS contains: lon, lat, weight, GISJOIN, tz
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
build_grid_weights <- function(cfg) {
  message("[grid] Building weights (area-based).")
  
  stopifnot(requireNamespace("arrow",        quietly = TRUE))
  stopifnot(requireNamespace("data.table",   quietly = TRUE))
  stopifnot(requireNamespace("sf",           quietly = TRUE))
  stopifnot(requireNamespace("terra",        quietly = TRUE))
  stopifnot(requireNamespace("raster",       quietly = TRUE))
  stopifnot(requireNamespace("exactextractr",quietly = TRUE))
  stopifnot(requireNamespace("lutz",         quietly = TRUE))
  
  # ---- Paths
  weights_rds <- file.path(
    cfg$out_dir, cfg$geog_name,
    sprintf("gridded_output_parquet-%s", cfg$date_version),
    sprintf("grid_to_geograhpy_dt-%s-%s.Rds", cfg$geog_name, cfg$date_version)
  )
  dir.create(dirname(weights_rds), recursive = TRUE, showWarnings = FALSE)
  
  # ---- Geography (WGS84) and optional state filter
  g <- sf::st_read(cfg$geog_shp, quiet = TRUE)
  g <- sf::st_transform(g, 4326)
  if (!is.null(cfg$state_fips) && nzchar(cfg$state_fips) && "STATEFP" %in% names(g)) {
    sf_code <- sprintf("%02d", as.integer(cfg$state_fips))
    g <- g[g$STATEFP == sf_code, ]
  }
  if (!"GISJOIN" %in% names(g)) stop("Shapefile must contain a 'GISJOIN' column.")
  g$GISJOIN <- as.character(g$GISJOIN)
  
  # ---- Open full dataset (not a single file)
  ds <- arrow::open_dataset(cfg$data_dir, format = "parquet")
  sch <- names(arrow::schema(ds))
  
  
  # right before pick_idx_col() calls in build_grid_weights()
  normalize_idx_name <- function(x) {
    if (is.null(x) || !nzchar(x)) return(x)
    sub("_x10$", "_10", x)   # map the common variant to actual
  }
  
  cfg$lat_col <- normalize_idx_name(cfg$lat_col)
  cfg$lon_col <- normalize_idx_name(cfg$lon_col)
  

  
  # ---- Resolve index column names
  pick_idx_col <- function(prefer, schema_names, patterns) {
    if (!is.null(prefer) && nzchar(prefer) && prefer %in% schema_names) return(prefer)
    # try patterns in order
    for (rx in patterns) {
      hits <- grep(rx, schema_names, value = TRUE, ignore.case = TRUE)
      # prefer canonical "lat_idx_10" / "lon_idx_10" if present
      if (length(hits)) {
        ord <- match(c("lat_idx_10","lon_idx_10","lat_idx_x10","lon_idx_x10"), hits)
        if (any(!is.na(ord))) hits <- hits[order(replace(ord, is.na(ord), 999))]
        return(hits[1])
      }
    }
    return(NA_character_)
  }
  
  idx_lat_col <- pick_idx_col(
    cfg$lat_col, sch,
    patterns = c("^lat_idx(_x?10)?$", "lat_idx", "lat.*idx", "^lat$")
  )
  idx_lon_col <- pick_idx_col(
    cfg$lon_col, sch,
    patterns = c("^lon_idx(_x?10)?$", "lon_idx", "lon.*idx", "long.*idx", "^lon$")
  )
  
  if (is.na(idx_lat_col) || is.na(idx_lon_col)) {
    stop(
      "Required index columns not found. ",
      "Looked for cfg$lat_col='", cfg$lat_col, "' / cfg$lon_col='", cfg$lon_col, "'; ",
      "schema columns include: ", paste(head(sch, 40), collapse = ", "), " ."
    )
  }
  
  message("[grid] Using index columns: lat='", idx_lat_col, "'  lon='", idx_lon_col, "'")
  message("[grid] Using scales: lat_scale=", cfg$lat_scale, "  lon_scale=", cfg$lon_scale)
  
  # ---- Partition pruning if available (optional; grid is static so one month is enough)
  if ("year" %in% sch && length(cfg$years)) {
    ds <- dplyr::filter(ds, year %in% cfg$years)
  }
  if ("month" %in% sch && !is.null(cfg$ref_month) && !is.na(cfg$ref_month)) {
    ds <- dplyr::filter(ds, month == cfg$ref_month)
  }
  
  # ---- Unique index pairs (Arrow ??? compute ??? collect)
  t0 <- proc.time()[["elapsed"]]
  idx_pairs <- ds %>%
    dplyr::select(dplyr::all_of(c(idx_lat_col, idx_lon_col))) %>%
    dplyr::distinct() %>%
    dplyr::compute() %>%
    dplyr::collect()
  t1 <- proc.time()[["elapsed"]]
  message(sprintf("[grid] distinct() returned %s rows in %.1fs",
                  format(nrow(idx_pairs), big.mark=","), t1 - t0))
  
  if (!nrow(idx_pairs)) stop("No index rows found-check data_dir / partition filters.")
  
  # ---- Convert indices ??? lat/lon once (NO get(); use [[ ]])
  idx_dt <- data.table::as.data.table(idx_pairs)
  ref <- data.table::data.table(
    lat = round(idx_dt[[idx_lat_col]] / cfg$lat_scale, 4),
    lon = round(idx_dt[[idx_lon_col]] / cfg$lon_scale, 4)
  )
  ref <- unique(ref)
  message(sprintf("[grid] Unique grid points: %s", format(nrow(ref), big.mark=",")))
  
  # ---- Rasterize (single dummy band) for exactextractr
  r <- terra::rast(ref[, .(lon, lat, value = 1)], type = "xyz", crs = "EPSG:4326")
  r <- raster::raster(r)
  
  # ---- Coverage fractions
  message("[grid] exact_extract: computing coverage fractions .")
  ee <- exactextractr::exact_extract(r, g, include_xy = TRUE, include_cols = "GISJOIN")
  
  weights <- data.table::rbindlist(lapply(seq_along(ee), function(i) {
    data.table::data.table(
      lon     = ee[[i]]$x,
      lat     = ee[[i]]$y,
      weight  = ee[[i]]$coverage_fraction,
      GISJOIN = g$GISJOIN[i]
    )
  }), fill = TRUE)
  
  weights <- weights[!is.na(lon) & !is.na(lat) & weight > 0]
  weights[, `:=`(lon = round(lon, 4), lat = round(lat, 4))]
  
  # ---- Time zone (mode per polygon), optional but useful later
  pts_sf <- sf::st_as_sf(weights[, .(lon, lat)], coords = c("lon", "lat"), crs = 4326)
  tz_vec <- lutz::tz_lookup(pts_sf, method = "fast", warn = FALSE)
  weights[, tz := tz_vec]
  tz_poly <- weights[, .N, by = .(GISJOIN, tz)][order(GISJOIN, -N)][, .SD[1], by = GISJOIN][, .(GISJOIN, tz)]
  
  # ---- Save
  saveRDS(weights, weights_rds)
  saveRDS(tz_poly, sub("\\.Rds$", "_tz.Rds", weights_rds))
  message(sprintf("[grid] Weights saved: %s (total %.1fs)", weights_rds, proc.time()[["elapsed"]] - t0))
  return(weights_rds)
}

# ???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
# Helpers
# ???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
`%||%` <- function(a,b) if (!is.null(a)) a else b

round_latlon <- function(x) round(x, 4)

arrow_time_window <- function(ds, time_col = "validTime",
                              start_posixct, end_posixct) {
  # Works for timestamp[us, tz=UTC] columns
  requireNamespace("arrow")
  tc <- arrow::Expression$field(time_col)
  start_ts <- arrow::Expression$scalar(as.POSIXct(start_posixct, tz = "UTC"))
  end_ts   <- arrow::Expression$scalar(as.POSIXct(end_posixct,   tz = "UTC"))
  ds %>% dplyr::filter(tc >= start_ts, tc < end_ts)
}

# Robust vars parser: "name=wbgt,col=wbgt_scaled10_degC,scale=10,unit=degC;name=ta,..."
parse_vars <- function(spec) {
  if (is.null(spec) || !nzchar(spec)) {
    stop("[vars] You must supply --vars. Example:\n",
         "  --vars=\"name=wbgt,col=wbgt_scaled10_degC,scale=10,unit=degC;name=ta,col=ta_scaled10_degC,scale=10,unit=degC\"")
  }
  parts <- strsplit(spec, ";", fixed = TRUE)[[1]]
  rows <- lapply(parts, function(p) {
    kv <- strsplit(p, ",", fixed = TRUE)[[1]]
    kvs <- strsplit(kv, "=", fixed = TRUE)
    lst <- setNames(lapply(kvs, `[`, 2), vapply(kvs, `[`, "", 1))
    list(
      name  = lst[["name"]]  %||% "",
      col   = lst[["col"]]   %||% "",
      scale = as.numeric(lst[["scale"]] %||% "1"),
      unit  = lst[["unit"]]  %||% ""
    )
  })
  dt <- data.table::rbindlist(rows, fill = TRUE)
  # Normalize
  dt[, name  := as.character(name)]
  dt[, col   := as.character(col)]
  dt[, scale := as.numeric(scale)]
  dt[, unit  := as.character(unit)]
  # Basic sanity
  bad <- dt[!nzchar(name) | !nzchar(col)]
  if (nrow(bad)) {
    warning(sprintf("[vars] Dropping %d malformed entries (need non-empty name & col):\n%s",
                    nrow(bad), paste(capture.output(print(bad)), collapse="\n")))
    dt <- dt[nzchar(name) & nzchar(col)]
  }
  if (!nrow(dt)) stop("[vars] After parsing, no valid variables remain.")
  dt
}

# ???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
# HOURLY STAGE (hardened)
# ???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
process_hourly <- function(cfg, weights_rds) {
  message("[hourly] Using weights: ", weights_rds)
  
  # 0) Load weights (lat, lon, weight, GISJOIN) and tz map
  weights <- readRDS(weights_rds)
  if (!all(c("lat","lon","weight","GISJOIN") %in% names(weights))) {
    stop("[hourly] weights RDS is missing required columns: lat, lon, weight, GISJOIN")
  }
  tz_rds <- sub("\\.Rds$", "_tz.Rds", weights_rds)
  tz_map <- if (file.exists(tz_rds)) readRDS(tz_rds) else NULL
  
  # 1) Vars: log and verify
  var_defs <- cfg$vars
  message("[hourly] Variables to aggregate:\n",
          paste(sprintf("  - %s := %s (scale=%s, unit=%s)",
                        var_defs$name, var_defs$col, var_defs$scale, var_defs$unit),
                collapse = "\n"))
  
  # 2) Open Arrow dataset once (root). We'll filter by time window per month.
  ds_root <- arrow::open_dataset(cfg$data_dir, format = "parquet")
  
  # Try to detect actual index column names in schema if user's choice isn't present.
  schema_names <- names(arrow::schema(ds_root))
  use_lat_col <- if (cfg$lat_col %in% schema_names) cfg$lat_col else {
    cand <- grep("^lat_idx", schema_names, value = TRUE)
    if (length(cand)) {
      msg <- sprintf("[hourly] cfg$lat_col='%s' not found; using '%s'", cfg$lat_col, cand[1])
      warning(msg)
      cand[1]
    } else {
      stop("[hourly] Could not find any lat index column in dataset. ",
           "Searched for '", cfg$lat_col, "' or ^lat_idx.")
    }
  }
  use_lon_col <- if (cfg$lon_col %in% schema_names) cfg$lon_col else {
    cand <- grep("^lon_idx", schema_names, value = TRUE)
    if (length(cand)) {
      msg <- sprintf("[hourly] cfg$lon_col='%s' not found; using '%s'", cfg$lon_col, cand[1])
      warning(msg)
      cand[1]
    } else {
      stop("[hourly] Could not find any lon index column in dataset. ",
           "Searched for '", cfg$lon_col, "' or ^lon_idx.")
    }
  }
  message(sprintf("[hourly] Using index columns: lat='%s'  lon='%s'", use_lat_col, use_lon_col))
  message(sprintf("[hourly] Using scales: lat_scale=%s  lon_scale=%s", cfg$lat_scale, cfg$lon_scale))
  
  # 3) Loop year-month
  for (yr in cfg$years) {
    message("[hourly] Year ", yr)
    for (mm in cfg$months) {
      message(sprintf("  . Month %02d", mm))
      
      # Time window (UTC)
      start <- lubridate::ymd_hms(sprintf("%04d-%02d-01 00:00:00", yr, mm), tz = "UTC")
      next_month <- if (mm == 12) 1 else (mm + 1)
      next_year  <- if (mm == 12) (yr + 1) else yr
      end   <- lubridate::ymd_hms(sprintf("%04d-%02d-01 00:00:00", next_year, next_month), tz = "UTC")
      
      # Columns we need
      needed_cols <- unique(c(use_lat_col, use_lon_col, "validTime", as.character(var_defs$col)))
      have <- needed_cols[needed_cols %in% schema_names]
      missing_now <- setdiff(needed_cols, have)
      if (length(missing_now)) {
        warning("[hourly] Skipping ", yr, "-", sprintf("%02d", mm),
                " - missing columns in dataset: ", paste(missing_now, collapse=", "))
        next
      }
      
      # Pull month window
      dat <- ds_root %>%
        dplyr::select(dplyr::all_of(have)) %>%
        arrow_time_window(time_col = "validTime", start_posixct = start, end_posixct = end) %>%
        dplyr::collect()
      
      if (!nrow(dat)) {
        message("    . no rows in Arrow window; skipping.")
        next
      }
      
      data.table::setDT(dat)
      # Convert scaled indices ??? real coords
      dat[, lat := round_latlon(.SD[[use_lat_col]] / cfg$lat_scale)]
      dat[, lon := round_latlon(.SD[[use_lon_col]] / cfg$lon_scale)]
      
      # Join weights (area-based). Allow cartesian (many grid points per polygon).
      data.table::setkeyv(dat, c("lat","lon"))
      ww <- data.table::copy(weights)
      data.table::setkeyv(ww, c("lat","lon"))
      m <- ww[dat, on = .(lat, lon), nomatch = 0L]  # m has: weight, GISJOIN + all dat cols
      
      if (!nrow(m)) {
        warning("    . grid join produced 0 rows; check lat/lon rounding/scales.")
        next
      }
      
      # Aggregate per var
      out_list <- list()
      by_keys <- c("GISJOIN","validTime")
      
      for (i in seq_len(nrow(var_defs))) {
        col_src <- as.character(var_defs$col[i])
        alias   <- as.character(var_defs$name[i])
        scl     <- as.numeric(var_defs$scale[i] %||% 1)
        
        if (!nzchar(col_src)) {
          warning(sprintf("    [var:%s] empty 'col' value; skipping", alias))
          next
        }
        if (!(col_src %in% names(m))) {
          warning(sprintf("    [var:%s] column '%s' not found in collected data; skipping", alias, col_src))
          next
        }
        
        # Weighted mean across grid cells per (GISJOIN, validTime)
        tmp <- m[, .(val = .SD[[col_src]] / scl, w = weight), .SDcols = col_src]
        tmp[, `:=`(GISJOIN = m$GISJOIN, validTime = m$validTime)]
        agg <- tmp[, .(mean = sum(val * w, na.rm = TRUE) /
                         sum(w[!is.na(val)], na.rm = TRUE)), by = by_keys]
        data.table::setnames(agg, "mean", paste0(alias, "_mean"))
        out_list[[alias]] <- agg
      }
      
      if (!length(out_list)) {
        warning("    . no variables aggregated for this month; skipping write.")
        next
      }
      
      # Merge all per-var tables and attach tz if available
      out_dt <- Reduce(function(a, b) merge(a, b, by = by_keys, all = TRUE), out_list)
      if (!is.null(tz_map) && all(c("GISJOIN","tz") %in% names(tz_map))) {
        out_dt <- merge(out_dt, tz_map, by = "GISJOIN", all.x = TRUE)
      }
      
      # Write hourly parquet
      out_dir_hourly <- file.path(cfg$out_dir, cfg$geog_name,
                                  sprintf("gridded_output_parquet-%s", cfg$date_version),
                                  "hourly", sprintf("%04d", yr))
      dir.create(out_dir_hourly, recursive = TRUE, showWarnings = FALSE)
      out_file <- file.path(out_dir_hourly,
                            sprintf("hourly_%d_%02d_%s.parquet", yr, mm, cfg$geog_name))
      arrow::write_parquet(out_dt, out_file)
      message("    ??? wrote: ", out_file)
    }
  }
}

# ---------------------------------
# STEP 3: Daily aggregation
# ---------------------------------
process_daily <- function(cfg) {
  message("[daily] Start")
  hourly_root <- file.path(
    cfg$out_dir, cfg$geog_name,
    sprintf("gridded_output_parquet-%s", cfg$date_version),
    "hourly"
  )
  daily_root <- file.path(
    cfg$out_dir, cfg$geog_name,
    sprintf("gridded_output_parquet-%s", cfg$date_version),
    "daily"
  )
  ensure_dirs(daily_root)
  
  mean_cols <- paste0(cfg$vars$name, "_mean")
  
  for (yr in cfg$years) {
    message("[daily] Year ", yr)
    ds_hourly <- open_dataset(hourly_root, format = "parquet")
    
    for (mm in cfg$months) {
      month_start <- ymd_hms(sprintf("%04d-%02d-01 00:00:00", yr, mm), tz="UTC")
      next_month  <- if (mm == 12) 1 else (mm + 1)
      next_year   <- if (mm == 12) (yr + 1) else yr
      month_end   <- ymd_hms(sprintf("%04d-%02d-01 00:00:00", next_year, next_month), tz="UTC")
      
      needed <- unique(c("GISJOIN","tz","validTime","localdate", mean_cols))
      dt <- ds_hourly %>%
        dplyr::select(dplyr::all_of(needed)) %>%
        arrow_time_window("validTime", month_start, month_end) %>%
        collect()
      
      if (!nrow(dt)) {
        message("  [daily] no rows for ", yr, "-", sprintf("%02d", mm))
        next
      }
      setDT(dt)
      
      by_keys <- c("localdate","GISJOIN")
      out_dt <- unique(dt[, ..by_keys])
      
      # daily stats for each mean column
      for (alias in cfg$vars$name) {
        cm <- paste0(alias, "_mean")
        if (!(cm %in% names(dt))) next
        agg <- dt[, .(
          val_max = round(max(get(cm), na.rm = TRUE), 1),
          val_min = round(min(get(cm), na.rm = TRUE), 1),
          val_mean= round(mean(get(cm), na.rm = TRUE), 1)
        ), by = by_keys]
        setnames(agg, c(by_keys, paste0(alias, c("_max","_min","_mean"))))
        out_dt <- merge(out_dt, agg, by = by_keys, all.x = TRUE)
      }
      
      out_y <- file.path(daily_root, yr)
      ensure_dirs(out_y)
      out_file <- file.path(out_y, sprintf("%d_%02d-%s-dailystats.parquet", yr, mm, cfg$geog_name))
      write_parquet(out_dt, out_file)
      message("  [daily] ??? ", out_file)
    }
  }
  invisible(TRUE)
}

# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
# CLI parsing (base R)
# ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
arg <- commandArgs(trailingOnly = TRUE)
get_flag <- function(flag, default = NULL) {
  hit <- grep(paste0("^", flag, "="), arg, value = TRUE)
  if (length(hit)) sub(paste0("^", flag, "="), "", hit[1]) else default
}
must <- function(x, msg) { if (is.null(x) || !nzchar(x)) stop(msg); x }

main <- function() {
  out_dir   <- must(get_flag("--out"),         "--out is required")
  data_dir  <- must(get_flag("--data-dir"),    "--data-dir is required")
  geog_shp  <- get_flag("--geog-shp", "")
  geog_name <- must(get_flag("--geog-name"),   "--geog-name is required")
  state_fp  <- get_flag("--state-fips", "")
  years_v   <- must(get_flag("--years"),       "--years is required")
  months_v  <- must(get_flag("--months"),      "--months is required")
  vars_s    <- must(get_flag("--vars"),        "--vars is required")
  lat_col   <- must(get_flag("--lat-col"),     "--lat-col is required")
  lon_col   <- must(get_flag("--lon-col"),     "--lon-col is required")
  lat_scale <- as.numeric(must(get_flag("--lat-scale"), "--lat-scale is required"))
  lon_scale <- as.numeric(must(get_flag("--lon-scale"), "--lon-scale is required"))
  
  years  <- as.integer(unlist(strsplit(years_v,  ",")))
  months <- as.integer(unlist(strsplit(months_v, ",")))
  vars_dt <- parse_vars(vars_s)
  
  cfg <- list(
    out_dir = normalizePath(out_dir, winslash="/", mustWork=FALSE),
    data_dir = normalizePath(data_dir, winslash="/", mustWork=TRUE),
    geog_shp = geog_shp,
    geog_name = geog_name,
    state_fips = state_fp,
    years = years,
    months = months,
    vars = vars_dt,
    lat_col = lat_col,
    lon_col = lon_col,
    lat_scale = lat_scale,
    lon_scale = lon_scale,
    date_version = format(Sys.Date(), "%Y%m"),
    ref_month = 5
  )
  
  # Build weights if a shapefile was provided; otherwise reuse existing RDS path
  weights_rds <- get_flag("--weights-rds", "")
  if (nzchar(geog_shp)) {
    weights_rds <- build_grid_weights(cfg)
  } else if (!nzchar(weights_rds)) {
    # default location (if user already generated weights before)
    weights_rds <- file.path(
      cfg$out_dir, cfg$geog_name,
      sprintf("gridded_output_parquet-%s", cfg$date_version),
      sprintf("grid_to_geograhpy_dt-%s-%s.Rds", cfg$geog_name, cfg$date_version)
    )
    if (!file.exists(weights_rds)) {
      stop("Weights RDS not found. Provide --geog-shp to build, or --weights-rds=<path> to reuse.")
    }
  }
  
  process_hourly(cfg, weights_rds)
  process_daily(cfg)
}

if (identical(environment(), globalenv()) && !length(sys.calls())) {
  tryCatch(main(), error = function(e) {
    message("\nERROR: ", conditionMessage(e))
    quit(status = 1)
  })
}
